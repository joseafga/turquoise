require "json"
require "./types.cr"

module Turquoise
  class Eloquent
    module Chat
      # The base structured datatype containing multi-part content of a message.
      # See: https://ai.google.dev/api/caching#Content
      struct Content
        include JSON::Serializable
        # Optional. The producer of the content.
        property role : Role?
        # Ordered Parts that constitute a single message. Parts may have different MIME types.
        property parts = [] of Part
        # Internal handle to send images using telegram API
        @[JSON::Field(ignore: true)]
        property photo : String | File | Nil

        def initialize(text : String? = nil, @role = nil, @photo = nil)
          @parts << Part.new(text) unless text.nil?
        end

        # Markdown special character escaping
        def escape_md
          Helpers.escape_md to_s
        end

        def to_s
          io = IO::Memory.new

          parts.each do |part|
            next if part.text.nil?
            io << part.text.to_s << '\n'
          end

          io.to_s
        end

        enum Role
          User
          Model
        end

        # A datatype containing media that is part of a multi-part `Content` message.
        # See: https://ai.google.dev/api/caching#Part
        struct Part
          include JSON::Serializable
          # Inline text.
          property text : String?

          # TODO: another properties
          # property inline_data # Inline media bytes.
          # property function_call # A predicted FunctionCall returned from the model that contains a string representing the FunctionDeclaration.name with the arguments and their values.
          # property function_response # The result output of a FunctionCall that contains a string representing the FunctionDeclaration.name and a structured JSON object containing any output from the function is used as context to the model.
          # property file_data # URI based data.
          # property executable_code # Code generated by the model that is meant to be executed.
          # property code_execution_result # Result of executing the ExecutableCode.

          def initialize(@text = nil)
          end
        end
      end

      # See: https://ai.google.dev/api/generate-content#method:-models.generatecontent
      # TODO: tools, tools_config
      struct Request
        include JSON::Serializable
        # Optional. Developer set `system instruction(s)`. Currently, text only.
        # See: https://ai.google.dev/gemini-api/docs/system-instructions
        @[JSON::Field(key: "systemInstruction")]
        property system_instruction : Chat::Content?

        # Required. The content of the current conversation with the model.
        # For single-turn queries, this is a single instance. For multi-turn queries
        # like chat, this is a repeated field that contains the conversation history
        # and the latest request.
        property contents : Deque(Chat::Content)

        # Optional. A list of unique SafetySetting instances for blocking unsafe content.
        @[JSON::Field(key: "safetySettings")]
        property safety_settings : Array(SafetySetting)?

        # Optional. Configuration options for model generation and outputs.
        @[JSON::Field(key: "generationConfig")]
        property generation_config : GenerationConfig?

        # Optional. The name of the content [cached](https://ai.google.dev/gemini-api/docs/caching)
        # to use as context to serve the prediction. Format: cachedContents/{cachedContent}
        @[JSON::Field(key: "cachedContent")]
        property cached_content : String?

        def initialize(@system_instruction = nil)
          @contents = Deque(Chat::Content).new(MAX_MESSAGES)
          # @tools = [] of Tool
        end

        # Keep maximum size and system message
        def <<(message : Chat::Content)
          contents.shift if contents.size >= MAX_MESSAGES
          contents.push message
        end

        # Configuration options for model generation and outputs. Not all parameters
        # are configurable for every model.
        # See: https://ai.google.dev/api/generate-content#generationconfig
        struct GenerationConfig
          include JSON::Serializable
          # Number of generated responses to return.
          @[JSON::Field(key: "candidateCount")]
          property candidate_count : Int32?

          # The set of character sequences (up to 5) that will stop output generation.
          # If specified, the API will stop at the first appearance of a stop sequence.
          # The stop sequence will not be included as part of the response.
          @[JSON::Field(key: "stopSequences")]
          property stop_sequences : Array(String)?

          # Controls the randomness of the output.
          # Values can range from [0.0,1.0], inclusive. A value closer to 1.0 will
          # produce responses that are more varied and creative, while a value closer
          # to 0.0 will typically result in more straightforward responses from the
          # model.
          property temperature : Float64?

          # The maximum number of tokens to include in a candidate.
          # If unset, this will default to output_token_limit specified in the model's
          # specification.
          @[JSON::Field(key: "maxOutputTokens")]
          property max_output_tokens : Int32?

          # The maximum number of tokens to consider when sampling.
          # The model uses combined Top-k and nucleus sampling.
          # Top-k sampling considers the set of top_k most probable tokens. Defaults to 40.
          @[JSON::Field(key: "topK")]
          property top_k : Int32?

          # The maximum cumulative probability of tokens to consider when sampling.
          # The model uses combined Top-k and nucleus sampling.
          # Tokens are sorted based on their assigned probabilities so that only the
          # most likely tokens are considered. Top-k sampling directly limits the
          # maximum number of tokens to consider, while Nucleus sampling limits number
          # of tokens based on the cumulative probability.
          @[JSON::Field(key: "topP")]
          property top_p : Float64?

          # Output response mimetype of the generated candidate text.
          # Supported mimetype:
          # - `text/plain`: (default) Text output.
          # - `application/json`: JSON response in the candidates.
          @[JSON::Field(key: "responseMimeType")]
          property response_mime_type : String?

          # Specifies the format of the JSON requested if response_mime_type is
          # `application/json`.
          @[JSON::Field(key: "responseSchema")]
          property response_schema : Nil # TODO

          def initialize(
            @stop_sequences = nil,
            @temperature = nil,
            @max_output_tokens = nil,
            @top_k = nil,
            @top_p = nil
          )
          end
        end
      end

      # Response from the model supporting multiple candidate responses.
      # See: https://ai.google.dev/api/generate-content#generatecontentresponse
      # TODO: promptFeedback
      struct Result
        include JSON::Serializable
        # Candidate responses from the model.
        property candidates : Array(Candidate)

        # Returns the prompt's feedback related to the content filters.
        @[JSON::Field(key: "promptFeedback")]
        property prompt_feedback : PromptFeedback?

        # Output only. Metadata on the generation requests' token usage.
        @[JSON::Field(key: "usageMetadata")]
        getter usage_metadata : UsageMetadata?

        @[JSON::Field(key: "modelVersion")]
        getter model_version : String?

        # A response candidate generated from the model.
        # See: https://ai.google.dev/api/generate-content#candidate
        struct Candidate
          include JSON::Serializable
          # Output only. Generated content returned from the model.
          property content : Chat::Content?

          # Optional. Output only. The reason why the model stopped generating tokens.
          # If empty, the model has not stopped generating tokens.
          @[JSON::Field(key: "finishReason")]
          property finish_reason : FinishReason?

          # List of ratings for the safety of a response candidate.
          # There is at most one rating per category.
          @[JSON::Field(key: "safetyRatings")]
          property safety_ratings : Array(SafetyRating)

          # TODO: citationMetadata - object (CitationMetadata)
          # Output only. Citation information for model-generated candidate.
          # This field may be populated with recitation information for any text included in the content. These are passages that are "recited" from copyrighted material in the foundational LLM's training data.

          # Output only. Token count for this candidate.
          @[JSON::Field(key: "tokenCount")]
          property token_count : Int32 = 0

          # TODO: avgLogprobs - number
          # Output only.
          # TODO: logprobsResult - object (LogprobsResult)
          # Output only. Log-likelihood scores for the response tokens and top tokens

          # Output only. Index of the candidate in the list of response candidates.
          property index : Int32 = 0
        end

        # A set of the feedback metadata the prompt specified in `Turquoise::Eloquent::Chat::Content`.
        # See: https://ai.google.dev/api/generate-content#PromptFeedback
        struct PromptFeedback
          include JSON::Serializable
          # Optional. If set, the prompt was blocked and no candidates are returned.
          @[JSON::Field(key: "blockReason")]
          property block_reason : BlockReason?

          # Ratings for safety of the prompt. There is at most one rating per category.
          @[JSON::Field(key: "safetyRatings")]
          property safety_ratings : Array(SafetyRating)
        end

        # Metadata on the generation request's token usage.
        # See: https://ai.google.dev/api/generate-content#UsageMetadata
        struct UsageMetadata
          include JSON::Serializable
          # Number of tokens in the prompt. When cachedContent is set, this is still
          # the total effective prompt size meaning this includes the number of tokens
          # in the cached content.
          @[JSON::Field(key: "promptTokenCount")]
          getter prompt_token_count : Int32?

          # Number of tokens in the cached part of the prompt (the cached content)
          @[JSON::Field(key: "cachedContentTokenCount")]
          getter cached_content_token_count : Int32?

          # Total number of tokens across all the generated response candidates.
          @[JSON::Field(key: "candidatesTokenCount")]
          getter candidates_token_count : Int32?

          # Total token count for the generation request (prompt + response candidates).
          @[JSON::Field(key: "totalTokenCount")]
          getter total_token_count : Int32
        end
      end

      # Simplified error response
      struct Error
        include JSON::Serializable
        property code : Int32
        property message : String
        property status : String
      end
    end

    module Prompt
      struct Request
        include JSON::Serializable
        property prompt : String
        property image : File?
        property mask : File?
        property num_steps : Int32?
        property strength : Int32?
        property guidance : Float32?

        def initialize(@prompt, @num_steps = nil)
        end
      end
    end
  end
end
